{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Análisis de grandes volúmenes de datos (Gpo 10)**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "## Actividad 3\n",
        "\n",
        "###  Aprendizaje supervisado y no supervisado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombrey matrícula**\n",
        "\n",
        "*   **A01746998** - Alexys Martín Coate Reyes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "# **1. Introducción teórica**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2x-zjDrIVWB"
      },
      "source": [
        "## **Aprendizaje Supervisado**\n",
        "\n",
        "El aprendizaje supervisado intenta predecir un output con base en una serie de datos de entrenamiento que vienen previamente clasificados de manera correcta. Este tipo de aprendizaje se asemeja al que tiene un niño pequeño que está aprendiendo a abstraer las características por medio de una guía, padre o profesor.\n",
        "\n",
        "Entre los algoritmos supervisados más populares están los siguientes:\n",
        "\n",
        "* Regresión Lineal\n",
        "  * ***pyspark.ml.regression.LinearRegression***\n",
        "* Regresión Logística\n",
        "  * ***pyspark.ml.classification.LogisticRegression***\n",
        "* Árboles de Decisión\n",
        "  * ***pyspark.ml.classification.DecisionTreeClassifier***\n",
        "  * ***pyspark.ml.regression.DecisionTreeRegressor***\n",
        "* Bosques Aleatorios (Random Forests)\n",
        "  * ***pyspark.ml.classification.RandomForestClassifier***\n",
        "  * ***pyspark.ml.regression.RandomForestRegressor***\n",
        "* Máquinas de Soporte Vectorial (SVM)\n",
        "  * ***pyspark.ml.classification.LinearSVC***\n",
        "* Gradient-Boosted Trees (GBTs)\n",
        "  * ***pyspark.ml.classification.GBTClassifier***\n",
        "  * ***pyspark.ml.regression.GBTRegressor***\n",
        "* Bayes Ingenuo (Naive Bayes)\n",
        "  * ***pyspark.ml.classification.NaiveBayes***\n",
        "\n",
        "\n",
        "## **Aprendizaje No Supervisado**\n",
        "\n",
        "Este tipo de modelos trabajan con un conjunto de datos no etiquetado, por lo que el mismo modelo aprende de manera automática los patrones y relaciones ocultas por si mismo.\n",
        "\n",
        "Comunmente se utilizan en problemas de \"clustering\", \"Reducción de dimensionalidad\" o \"Reglas de asociación\".\n",
        "\n",
        "Ejemplos de estos modelos son:\n",
        "\n",
        "* K-Means\n",
        "  * ***pyspark.ml.clustering.KMeans***\n",
        "* Análisis de Componentes Principales (PCA)\n",
        "  * ***pyspark.ml.feature.PCA***\n",
        "* Factorización de Matrices No Negativas (NMF)\n",
        "  * ***pyspark.ml.clustering.NNMF***\n",
        "* Gaussian Mixture Models (GMM)\n",
        "  * ***pyspark.ml.clustering.GaussianMixture***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfZZ0stLmWJN"
      },
      "source": [
        "# **2. Selección de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsOuyDEXIVWC"
      },
      "source": [
        "### Cargando los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "IgaTP9WVIVWC"
      },
      "outputs": [],
      "source": [
        "# Librerias\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFKKRFUWIVWC",
        "outputId": "5f83e443-565c-425a-9367-1b63ab74baa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- FL_DATE: date (nullable = true)\n",
            " |-- OP_CARRIER: string (nullable = true)\n",
            " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
            " |-- ORIGIN: string (nullable = true)\n",
            " |-- DEST: string (nullable = true)\n",
            " |-- CRS_DEP_TIME: integer (nullable = true)\n",
            " |-- DEP_TIME: double (nullable = true)\n",
            " |-- DEP_DELAY: double (nullable = true)\n",
            " |-- TAXI_OUT: double (nullable = true)\n",
            " |-- WHEELS_OFF: double (nullable = true)\n",
            " |-- WHEELS_ON: double (nullable = true)\n",
            " |-- TAXI_IN: double (nullable = true)\n",
            " |-- CRS_ARR_TIME: integer (nullable = true)\n",
            " |-- ARR_TIME: double (nullable = true)\n",
            " |-- ARR_DELAY: double (nullable = true)\n",
            " |-- CANCELLED: double (nullable = true)\n",
            " |-- DIVERTED: double (nullable = true)\n",
            " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
            " |-- ACTUAL_ELAPSED_TIME: double (nullable = true)\n",
            " |-- AIR_TIME: double (nullable = true)\n",
            " |-- DISTANCE: double (nullable = true)\n",
            "\n",
            "Número total de registros: 18505725\n"
          ]
        }
      ],
      "source": [
        "# Crear sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EDA_Vuelos\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Leer el CSV\n",
        "df = spark.read.csv(\"./Airline_Delay_2016-2018.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Mostrar esquema de datos\n",
        "df.printSchema()\n",
        "\n",
        "# Número total de registros\n",
        "total_registros = df.count()\n",
        "print(f\"Número total de registros: {total_registros}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEFAlxViIVWD",
        "outputId": "2ca050a7-3233-44f1-e40c-3b930118e053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEP_TIME|DEP_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|\n",
            "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+\n",
            "|2016-01-01|        DL|             1248|   DTW| LAX|        1935|  1935.0|      0.0|    23.0|    1958.0|   2107.0|   13.0|        2144|  2120.0|    -24.0|      0.0|     0.0|           309.0|              285.0|   249.0|  1979.0|\n",
            "|2016-01-01|        DL|             1251|   ATL| GRR|        2125|  2130.0|      5.0|    13.0|    2143.0|   2315.0|    4.0|        2321|  2319.0|     -2.0|      0.0|     0.0|           116.0|              109.0|    92.0|   640.0|\n",
            "|2016-01-01|        DL|             1254|   LAX| ATL|        2255|  2256.0|      1.0|    19.0|    2315.0|    542.0|    5.0|         600|   547.0|    -13.0|      0.0|     0.0|           245.0|              231.0|   207.0|  1947.0|\n",
            "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imprimiendo los 3 primeros rengloes del dataframe dataframe\n",
        "df.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZpuarHKIVWD"
      },
      "source": [
        "### Seleccionando variables de caracterización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v-xZ8FrIVWD",
        "outputId": "35109f0a-ff80-47e4-bf59-8b277e58b914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OP_CARRIER 18\n",
            "ORIGIN 362\n",
            "DEST 360\n",
            "CANCELLED 2\n",
            "DIVERTED 2\n"
          ]
        }
      ],
      "source": [
        "# Variables seleccionadas\n",
        "vars_particion = [\"OP_CARRIER\", \"ORIGIN\", \"DEST\", \"CANCELLED\", \"DIVERTED\"]\n",
        "\n",
        "# Imrpimiendo la cantidad de valores únicos que se tiene por las columnas seleccionadas\n",
        "for col in vars_particion:\n",
        "    print(col, df.select(col).distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLoGLStdIVWD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, count, round\n",
        "\n",
        "# Calcular frecuencia de combinaciones\n",
        "combinaciones = df.groupBy(vars_particion).count()\n",
        "\n",
        "# Total de registros\n",
        "total = df.count()\n",
        "\n",
        "# Agregar probabilidad\n",
        "combinaciones = combinaciones.withColumn(\"probabilidad\", col(\"count\") / total)\n",
        "\n",
        "combinaciones.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh9EGU1cIVWE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, lit\n",
        "\n",
        "# Definir tamaño total de la muestra (por ejemplo, 1% del total)\n",
        "tamaño_muestra_total = int(total * 0.01)\n",
        "\n",
        "# Establecer un mínimo de registros por partición\n",
        "minimo_por_particion = 0\n",
        "\n",
        "# Calcular el tamaño de muestra por partición según su probabilidad\n",
        "combinaciones_con_tamaño = combinaciones.withColumn(\n",
        "    \"tamaño_muestra\",\n",
        "    round(\n",
        "        when(\n",
        "            (col(\"probabilidad\") * tamaño_muestra_total) < minimo_por_particion,\n",
        "            lit(minimo_por_particion)\n",
        "        ).otherwise(col(\"probabilidad\") * tamaño_muestra_total)\n",
        "    ).cast(\"int\")\n",
        ")\n",
        "\n",
        "# Ordenar para visualizar mejor\n",
        "combinaciones_con_tamaño = combinaciones_con_tamaño.orderBy(col(\"tamaño_muestra\").desc())\n",
        "\n",
        "combinaciones_con_tamaño.show(10, truncate=False)\n",
        "\n",
        "# Calcular el tamaño final total de la muestra\n",
        "tamaño_muestra_final = combinaciones_con_tamaño.agg({\"tamaño_muestra\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "print(f\"Tamaño total esperado de la muestra final: {tamaño_muestra_final}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhNyVQlZIVWE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "# Unir el tamaño de muestra a cada combinación en el dataset original\n",
        "df_con_muestra = df.join(\n",
        "    combinaciones_con_tamaño.select(*vars_particion, \"tamaño_muestra\"),\n",
        "    on=vars_particion,\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Crear ventana ordenada por fecha y hora dentro de cada combinación\n",
        "ventana = Window.partitionBy(*vars_particion).orderBy(\"FL_DATE\", \"CRS_DEP_TIME\")\n",
        "\n",
        "# Enumerar vuelos por combinación (ordenados por tiempo)\n",
        "df_con_muestra = df_con_muestra.withColumn(\"row_num\", row_number().over(ventana))\n",
        "\n",
        "# Filtrar solo los primeros N registros por combinación\n",
        "muestra_final = df_con_muestra.filter(col(\"row_num\") <= col(\"tamaño_muestra\"))\n",
        "\n",
        "# Mostrar la muestra final\n",
        "muestra_final.show(10, truncate=False)\n",
        "\n",
        "print(f\"Tamaño total de la muestra: {muestra_final.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2laT-P9CIVWE"
      },
      "source": [
        "# **3. Preparación de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-1JqJZaIVWE"
      },
      "source": [
        "### Selección de variables importantes para el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btXgOJJsIVWE"
      },
      "source": [
        "Se realizará un análisis de predicción para saber que tanto un avión se retrasará. Por ello se quitarán las columnas que no aportan mucha información para el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxkaAUoSIVWE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pandas_df = muestra_final.limit(50).toPandas()\n",
        "display(pandas_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPnAvve5IVWE"
      },
      "outputs": [],
      "source": [
        "# Se crea una nueva columna para verificar si\n",
        "delayed_time = 10\n",
        "df_muestra = muestra_final.withColumn(\"DELAYED\", when(df.ARR_DELAY > delayed_time, 1).otherwise(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvc9ghddIVWE"
      },
      "outputs": [],
      "source": [
        "# Variables a descartar\n",
        "cols_to_drop = [\"tamaño_muestra\", \"row_num\", \"OP_CARRIER_FL_NUM\", \"DEP_TIME\",\"DEP_DELAY\", \"TAXI_OUT\",\n",
        "                \"WHEELS_OFF\", \"WHEELS_ON\", \"TAXI_IN\" , \"ARR_TIME\", \"ARR_DELAY\", \"DIVERTED\", \"ACTUAL_ELAPSED_TIME\", \"AIR_TIME\"]\n",
        "\n",
        "# Variables importantes para el modelo\n",
        "selected_cols = [\n",
        "    \"FL_DATE\", \"OP_CARRIER\", \"ORIGIN\", \"DEST\",\n",
        "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\",\n",
        "    \"DISTANCE\", \"CANCELLED\", \"ARR_DELAY\", \"DELAYED\"\n",
        "]\n",
        "\n",
        "# Realizando la selección de columnas importantes como datos de entrenamiento\n",
        "df_raw = df_muestra.select(*selected_cols)\n",
        "df_raw.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iag0yzPCIVWF"
      },
      "source": [
        "### Transformación de variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcXKZkGCIVWF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import dayofweek, month\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Extrayendo el día, mes y eliminando la columna original de fecha\n",
        "df_transformed = df_raw.withColumn(\"FL_DAY_OF_WEEK\", dayofweek(\"FL_DATE\")) \\\n",
        "                     .withColumn(\"FL_MONTH\", month(\"FL_DATE\")) \\\n",
        "                     .drop(\"FL_DATE\")\n",
        "\n",
        "# Codificaando las variables categóricas\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"OP_CARRIER_T\"),\n",
        "    StringIndexer(inputCol=\"ORIGIN\", outputCol=\"ORIGIN_T\"),\n",
        "    StringIndexer(inputCol=\"DEST\", outputCol=\"DEST_T\")\n",
        "]\n",
        "\n",
        "for indexer in indexers:\n",
        "    df_transformed = indexer.fit(df_transformed).transform(df_transformed)\n",
        "\n",
        "# Eliminando las columnas de las variables categóricas originales\n",
        "cols_to_drop = [\"OP_CARRIER\", \"ORIGIN\", \"DEST\"]\n",
        "df_transformed = df_transformed.drop(*cols_to_drop) # Using * to unpack the list of column names\n",
        "\n",
        "df_transformed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4gXvPhoIVWF"
      },
      "outputs": [],
      "source": [
        "# Guardando dataset en un archivo paquet para su posterior utilización\n",
        "df_transformed.write.mode(\"overwrite\").parquet(\"./df_transformed.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura del archivo que contiene el dataset de pyspark\n",
        "df_transformed = spark.read.parquet(\"./df_transformed.parquet\")"
      ],
      "metadata": {
        "id": "C96iZPM_JaKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8ywdYe4IVWF"
      },
      "outputs": [],
      "source": [
        "# Imprime los valores resultantes de la limpieza y transformación de los datos\n",
        "df_transformed.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkbHptzAIVWF"
      },
      "outputs": [],
      "source": [
        "# Imprime un resumen del dataframe con la limpieza de todos los datos\n",
        "df_transformed.describe().toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFpsQ-duIVWF"
      },
      "source": [
        "### Limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.groupBy(\"CANCELLED\").count().show()  # Cuenta cuantos vuelos cancelados hay\n",
        "df_transformed.groupBy(\"FL_MONTH\").count().show()   # Cuenta cuantos meses diferentes hay"
      ],
      "metadata": {
        "id": "z-idTlFtNZBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se elimina la columna de Cancelled y del mes ya que unicamente se cuenta con datos de un solo mes y no existen vuelos cancelasdos\n",
        "drop_cols = [\"CANCELLED\", \"FL_MONTH\"]\n",
        "df_transformed_2 = df_transformed.drop(*drop_cols)"
      ],
      "metadata": {
        "id": "PkOeCv64L13U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyjpnwBSIVWF"
      },
      "outputs": [],
      "source": [
        "#Se eliminan registros con valores nulos\n",
        "df_clean = df_transformed_2.dropna()\n",
        "\n",
        "#Se eliminan columnas con valores nulos\n",
        "df_clean = df_clean.na.drop()\n",
        "\n",
        "#Se eliminan registros duplicados\n",
        "df_clean = df_clean.dropDuplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKNQyIIyIVWF"
      },
      "outputs": [],
      "source": [
        "df_clean.describe().toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balanceo del dataset"
      ],
      "metadata": {
        "id": "64t09Cz0OU5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.groupBy(\"DELAYED\").count().show()   # Cuenta cuantos vuelos retrasados hay (Presencia de dataset desbalanceado)"
      ],
      "metadata": {
        "id": "0J92YLSsN14X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto indica que estamos trabajando con un dataset desbalanceado, por lo que debemos utlizar métodos para balancear el dataset."
      ],
      "metadata": {
        "id": "IkeOyHSyOdJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objetivo de balanceo\n",
        "target_size = 1125\n",
        "\n",
        "# Separar las clases\n",
        "df_majority = df_clean.filter(col(\"DELAYED\") == 0)\n",
        "df_minority = df_clean.filter(col(\"DELAYED\") == 1)\n",
        "\n",
        "# Submuestreo de clase mayoritaria (clase 0)\n",
        "df_majority_sampled = df_majority.sample(False, fraction=target_size / df_majority.count(), seed=42)\n",
        "\n",
        "# Sobremuestreo de clase minoritaria (clase 1)\n",
        "ratio = int(target_size / df_minority.count())\n",
        "df_minority_oversampled = df_minority\n",
        "for _ in range(ratio - 1):\n",
        "    df_minority_oversampled = df_minority_oversampled.union(df_minority)\n",
        "\n",
        "# Agregar una fracción adicional si no es exacto\n",
        "remaining = target_size - df_minority_oversampled.count()\n",
        "if remaining > 0:\n",
        "    df_minority_oversampled = df_minority_oversampled.union(\n",
        "        df_minority.sample(withReplacement=True, fraction=remaining / df_minority.count(), seed=42)\n",
        "    )\n",
        "\n",
        "# Unir los datasets balanceados\n",
        "df_balanced = df_majority_sampled.union(df_minority_oversampled)\n",
        "\n",
        "# Verificación\n",
        "df_balanced.groupBy(\"DELAYED\").count().show()"
      ],
      "metadata": {
        "id": "fT_fNmefOsQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resumen final del dataset balanceado\n",
        "df_balanced.describe().toPandas()"
      ],
      "metadata": {
        "id": "q4EG25IRSFAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjKoEqiqBN1"
      },
      "source": [
        "# **4. Prepraración del conjunto de entrenamiento y prueba**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modelo Supervisado**"
      ],
      "metadata": {
        "id": "ySZwwqGZHxjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOYsN3nWIVWG"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\n",
        "        \"OP_CARRIER_T\",\"ORIGIN_T\",\"DEST_T\",\n",
        "        \"CRS_DEP_TIME\",\"CRS_ARR_TIME\",\"CRS_ELAPSED_TIME\",\"DISTANCE\",\n",
        "        \"FL_DAY_OF_WEEK\",\n",
        "        \"ARR_DELAY\",\"DELAYED\"\n",
        "    ],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "df_vector = assembler.transform(df_balanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR7Q5XiCIVWG"
      },
      "outputs": [],
      "source": [
        "# Separación de datos de entrenamiento y prueba\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")       # Se define el valor por default del número de ejecutores\n",
        "df_train, df_val_test = df_vector.randomSplit([0.7, 0.3], seed=42)\n",
        "df_val, df_test = df_val_test.randomSplit([0.5, 0.5], seed=42)\n",
        "\n",
        "# Impresion del tamaño de las particiones\n",
        "print(f\"\"\"Total data: {df_vector.count()}\n",
        "Training data: {df_train.count()}\n",
        "Validation data: {df_val.count()}\n",
        "Test data: {df_test.count()}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modelo No Supervisado**"
      ],
      "metadata": {
        "id": "8DULKq4ZH32z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxCv-ftxH7ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS0Hxj25vTWh"
      },
      "source": [
        "# **5. Construcción de modelos de aprendizaje supervisado y no supervisado**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX60ksA0IVWG"
      },
      "source": [
        "### Modelo Supervisado - Regresión Logística"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenando el modelo"
      ],
      "metadata": {
        "id": "yId8-rsMGwBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhNFpf3EIVWG"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "# Inicializar el modelo GBTClassifier\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"DELAYED\", maxIter=10) # maxIter es un parámetro de ejemplo, puedes ajustarlo\n",
        "\n",
        "# Entrenar el modelo\n",
        "gbt_model = gbt.fit(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluando el modelo"
      ],
      "metadata": {
        "id": "GEI1NMm_G2Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = gbt_model.transform(df_test)\n",
        "\n",
        "# Inicializar el evaluador\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"DELAYED\")\n",
        "\n",
        "# Calcular el área bajo la curva ROC (AUC)\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Área bajo la curva ROC (AUC): {auc}\")"
      ],
      "metadata": {
        "id": "JCzIilt-GgdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6KLGGoEIVWG"
      },
      "source": [
        "### Modelo No Supervisado - K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenando el modelo"
      ],
      "metadata": {
        "id": "GY-gJF8LG7Oj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTJPbYoRIVWG"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Define el número de clústeres (puedes experimentar con diferentes valores)\n",
        "k = 5\n",
        "\n",
        "# Inicializa el modelo K-Means\n",
        "kmeans = KMeans(featuresCol=\"features\").setK(k).setSeed(1)\n",
        "\n",
        "# Entrena el modelo\n",
        "model = kmeans.fit(df_vector_clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluando el modelo"
      ],
      "metadata": {
        "id": "pfITbSIpHW_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza predicciones (asigna cada fila a un clúster)\n",
        "predictions = model.transform(df_vector_clustering)\n",
        "\n",
        "# Mostrar algunas predicciones\n",
        "predictions.select(\"features\", \"prediction\").show(10)\n",
        "\n",
        "# Evaluar el modelo usando Silhouette Score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(f\"Silhouette with squared Euclidean distance = {silhouette}\")\n",
        "\n",
        "# Muestra los centros de los clústeres\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "metadata": {
        "id": "-G0ILV_WHWtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5IHf_RZIVWG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mvYLYgAvHkFc"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4c34ZOnna3Gu",
        "MfZZ0stLmWJN",
        "ygchEdcKqIzU",
        "1qjKoEqiqBN1",
        "RS0Hxj25vTWh",
        "ToqRl7fT_fn2",
        "W4S7q0yR0Mpi",
        "pibp1LA91CP_",
        "WDIiSHvg0_hm",
        "NbhBUBKJp1MB",
        "YCkh2WfN1MC1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}