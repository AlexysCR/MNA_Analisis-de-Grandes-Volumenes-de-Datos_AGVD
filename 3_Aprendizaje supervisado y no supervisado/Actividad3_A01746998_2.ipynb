{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Análisis de grandes volúmenes de datos (Gpo 10)**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "## Actividad 3\n",
        "\n",
        "###  Aprendizaje supervisado y no supervisado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombrey matrícula**\n",
        "\n",
        "*   **A01746998** - Alexys Martín Coate Reyes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "# **1. Introducción teórica**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2x-zjDrIVWB"
      },
      "source": [
        "## **Aprendizaje Supervisado**\n",
        "\n",
        "El aprendizaje supervisado intenta predecir un output con base en una serie de datos de entrenamiento que vienen previamente clasificados de manera correcta. Este tipo de aprendizaje se asemeja al que tiene un niño pequeño que está aprendiendo a abstraer las características por medio de una guía, padre o profesor.\n",
        "\n",
        "Entre los algoritmos supervisados más populares están los siguientes:\n",
        "\n",
        "* Regresión Lineal\n",
        "  * ***pyspark.ml.regression.LinearRegression***\n",
        "* Regresión Logística\n",
        "  * ***pyspark.ml.classification.LogisticRegression***\n",
        "* Árboles de Decisión\n",
        "  * ***pyspark.ml.classification.DecisionTreeClassifier***\n",
        "  * ***pyspark.ml.regression.DecisionTreeRegressor***\n",
        "* Bosques Aleatorios (Random Forests)\n",
        "  * ***pyspark.ml.classification.RandomForestClassifier***\n",
        "  * ***pyspark.ml.regression.RandomForestRegressor***\n",
        "* Máquinas de Soporte Vectorial (SVM)\n",
        "  * ***pyspark.ml.classification.LinearSVC***\n",
        "* Gradient-Boosted Trees (GBTs)\n",
        "  * ***pyspark.ml.classification.GBTClassifier***\n",
        "  * ***pyspark.ml.regression.GBTRegressor***\n",
        "* Bayes Ingenuo (Naive Bayes)\n",
        "  * ***pyspark.ml.classification.NaiveBayes***\n",
        "\n",
        "\n",
        "## **Aprendizaje No Supervisado**\n",
        "\n",
        "Este tipo de modelos trabajan con un conjunto de datos no etiquetado, por lo que el mismo modelo aprende de manera automática los patrones y relaciones ocultas por si mismo.\n",
        "\n",
        "Comunmente se utilizan en problemas de \"clustering\", \"Reducción de dimensionalidad\" o \"Reglas de asociación\".\n",
        "\n",
        "Ejemplos de estos modelos son:\n",
        "\n",
        "* K-Means\n",
        "  * ***pyspark.ml.clustering.KMeans***\n",
        "* Análisis de Componentes Principales (PCA)\n",
        "  * ***pyspark.ml.feature.PCA***\n",
        "* Factorización de Matrices No Negativas (NMF)\n",
        "  * ***pyspark.ml.clustering.NNMF***\n",
        "* Gaussian Mixture Models (GMM)\n",
        "  * ***pyspark.ml.clustering.GaussianMixture***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfZZ0stLmWJN"
      },
      "source": [
        "# **2. Selección de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsOuyDEXIVWC"
      },
      "source": [
        "### Cargando los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "IgaTP9WVIVWC"
      },
      "outputs": [],
      "source": [
        "# Librerias\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "LFKKRFUWIVWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "41631a7c-9d1d-4b17-f0aa-f7ec9673420b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "SparkSession$ does not exist in the JVM -- Trying to access a non-static member from a static context.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-e377e608433d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EDA_Vuelos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Leer el CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                     getattr(\n\u001b[0;32m--> 503\u001b[0;31m                         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SparkSession$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MODULE$\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m                     ).applyModifiableSettings(session._jsparkSession, self._options)\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1724\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JError\u001b[0m: SparkSession$ does not exist in the JVM -- Trying to access a non-static member from a static context."
          ]
        }
      ],
      "source": [
        "# Crear sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EDA_Vuelos\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Leer el CSV\n",
        "df = spark.read.csv(\"./Airline_Delay_2016-2018.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Mostrar esquema de datos\n",
        "df.printSchema()\n",
        "\n",
        "# Número total de registros\n",
        "total_registros = df.count()\n",
        "print(f\"Número total de registros: {total_registros}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEFAlxViIVWD"
      },
      "outputs": [],
      "source": [
        "# Imprimiendo los 3 primeros rengloes del dataframe dataframe\n",
        "df.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZpuarHKIVWD"
      },
      "source": [
        "### Seleccionando variables de caracterización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v-xZ8FrIVWD"
      },
      "outputs": [],
      "source": [
        "# Variables seleccionadas\n",
        "vars_particion = [\"OP_CARRIER\", \"ORIGIN\", \"DEST\", \"CANCELLED\", \"DIVERTED\"]\n",
        "\n",
        "# Imrpimiendo la cantidad de valores únicos que se tiene por las columnas seleccionadas\n",
        "for col in vars_particion:\n",
        "    print(col, df.select(col).distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLoGLStdIVWD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, count, round\n",
        "\n",
        "# Calcular frecuencia de combinaciones\n",
        "combinaciones = df.groupBy(vars_particion).count()\n",
        "\n",
        "# Total de registros\n",
        "total = df.count()\n",
        "\n",
        "# Agregar probabilidad\n",
        "combinaciones = combinaciones.withColumn(\"probabilidad\", col(\"count\") / total)\n",
        "\n",
        "combinaciones.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh9EGU1cIVWE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, lit\n",
        "\n",
        "# Definir tamaño total de la muestra (por ejemplo, 1% del total)\n",
        "tamaño_muestra_total = int(total * 0.01)\n",
        "\n",
        "# Establecer un mínimo de registros por partición\n",
        "minimo_por_particion = 0\n",
        "\n",
        "# Calcular el tamaño de muestra por partición según su probabilidad\n",
        "combinaciones_con_tamaño = combinaciones.withColumn(\n",
        "    \"tamaño_muestra\",\n",
        "    round(\n",
        "        when(\n",
        "            (col(\"probabilidad\") * tamaño_muestra_total) < minimo_por_particion,\n",
        "            lit(minimo_por_particion)\n",
        "        ).otherwise(col(\"probabilidad\") * tamaño_muestra_total)\n",
        "    ).cast(\"int\")\n",
        ")\n",
        "\n",
        "# Ordenar para visualizar mejor\n",
        "combinaciones_con_tamaño = combinaciones_con_tamaño.orderBy(col(\"tamaño_muestra\").desc())\n",
        "\n",
        "combinaciones_con_tamaño.show(10, truncate=False)\n",
        "\n",
        "# Calcular el tamaño final total de la muestra\n",
        "tamaño_muestra_final = combinaciones_con_tamaño.agg({\"tamaño_muestra\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "print(f\"Tamaño total esperado de la muestra final: {tamaño_muestra_final}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhNyVQlZIVWE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "# Unir el tamaño de muestra a cada combinación en el dataset original\n",
        "df_con_muestra = df.join(\n",
        "    combinaciones_con_tamaño.select(*vars_particion, \"tamaño_muestra\"),\n",
        "    on=vars_particion,\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Crear ventana ordenada por fecha y hora dentro de cada combinación\n",
        "ventana = Window.partitionBy(*vars_particion).orderBy(\"FL_DATE\", \"CRS_DEP_TIME\")\n",
        "\n",
        "# Enumerar vuelos por combinación (ordenados por tiempo)\n",
        "df_con_muestra = df_con_muestra.withColumn(\"row_num\", row_number().over(ventana))\n",
        "\n",
        "# Filtrar solo los primeros N registros por combinación\n",
        "muestra_final = df_con_muestra.filter(col(\"row_num\") <= col(\"tamaño_muestra\"))\n",
        "\n",
        "# Mostrar la muestra final\n",
        "muestra_final.show(10, truncate=False)\n",
        "\n",
        "print(f\"Tamaño total de la muestra: {muestra_final.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2laT-P9CIVWE"
      },
      "source": [
        "# **3. Preparación de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo Supervisado**"
      ],
      "metadata": {
        "id": "Npc9y26uIQKh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-1JqJZaIVWE"
      },
      "source": [
        "### Selección de variables importantes para el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btXgOJJsIVWE"
      },
      "source": [
        "Se realizará un análisis de predicción para saber que tanto un avión se retrasará. Por ello se quitarán las columnas que no aportan mucha información para el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxkaAUoSIVWE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pandas_df = muestra_final.limit(50).toPandas()\n",
        "display(pandas_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPnAvve5IVWE"
      },
      "outputs": [],
      "source": [
        "# Se crea una nueva columna para verificar si\n",
        "delayed_time = 10\n",
        "df_muestra = muestra_final.withColumn(\"DELAYED\", when(df.ARR_DELAY > delayed_time, 1).otherwise(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvc9ghddIVWE"
      },
      "outputs": [],
      "source": [
        "# Variables a descartar\n",
        "cols_to_drop = [\"tamaño_muestra\", \"row_num\", \"OP_CARRIER_FL_NUM\", \"DEP_TIME\",\"DEP_DELAY\", \"TAXI_OUT\",\n",
        "                \"WHEELS_OFF\", \"WHEELS_ON\", \"TAXI_IN\" , \"ARR_TIME\", \"ARR_DELAY\", \"DIVERTED\", \"ACTUAL_ELAPSED_TIME\", \"AIR_TIME\"]\n",
        "\n",
        "# Variables importantes para el modelo\n",
        "selected_cols = [\n",
        "    \"FL_DATE\", \"OP_CARRIER\", \"ORIGIN\", \"DEST\",\n",
        "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\",\n",
        "    \"DISTANCE\", \"CANCELLED\", \"ARR_DELAY\", \"DELAYED\"\n",
        "]\n",
        "\n",
        "# Realizando la selección de columnas importantes como datos de entrenamiento\n",
        "df_raw = df_muestra.select(*selected_cols)\n",
        "df_raw.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iag0yzPCIVWF"
      },
      "source": [
        "### Transformación de variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcXKZkGCIVWF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import dayofweek, month\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Extrayendo el día, mes y eliminando la columna original de fecha\n",
        "df_transformed = df_raw.withColumn(\"FL_DAY_OF_WEEK\", dayofweek(\"FL_DATE\")) \\\n",
        "                     .withColumn(\"FL_MONTH\", month(\"FL_DATE\")) \\\n",
        "                     .drop(\"FL_DATE\")\n",
        "\n",
        "# Codificaando las variables categóricas\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"OP_CARRIER_T\"),\n",
        "    StringIndexer(inputCol=\"ORIGIN\", outputCol=\"ORIGIN_T\"),\n",
        "    StringIndexer(inputCol=\"DEST\", outputCol=\"DEST_T\")\n",
        "]\n",
        "\n",
        "for indexer in indexers:\n",
        "    df_transformed = indexer.fit(df_transformed).transform(df_transformed)\n",
        "\n",
        "# Eliminando las columnas de las variables categóricas originales\n",
        "cols_to_drop = [\"OP_CARRIER\", \"ORIGIN\", \"DEST\"]\n",
        "df_transformed = df_transformed.drop(*cols_to_drop) # Using * to unpack the list of column names\n",
        "\n",
        "df_transformed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4gXvPhoIVWF"
      },
      "outputs": [],
      "source": [
        "# Guardando dataset en un archivo paquet para su posterior utilización\n",
        "df_transformed.write.mode(\"overwrite\").parquet(\"./df_transformed.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura del archivo que contiene el dataset de pyspark\n",
        "df_transformed = spark.read.parquet(\"./df_transformed.parquet\")"
      ],
      "metadata": {
        "id": "C96iZPM_JaKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8ywdYe4IVWF"
      },
      "outputs": [],
      "source": [
        "# Imprime los valores resultantes de la limpieza y transformación de los datos\n",
        "df_transformed.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkbHptzAIVWF"
      },
      "outputs": [],
      "source": [
        "# Imprime un resumen del dataframe con la limpieza de todos los datos\n",
        "df_transformed.describe().toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFpsQ-duIVWF"
      },
      "source": [
        "### Limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.groupBy(\"CANCELLED\").count().show()  # Cuenta cuantos vuelos cancelados hay\n",
        "df_transformed.groupBy(\"FL_MONTH\").count().show()   # Cuenta cuantos meses diferentes hay"
      ],
      "metadata": {
        "id": "z-idTlFtNZBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se elimina la columna de Cancelled y del mes ya que unicamente se cuenta con datos de un solo mes y no existen vuelos cancelasdos\n",
        "drop_cols = [\"CANCELLED\", \"FL_MONTH\"]\n",
        "df_transformed_2 = df_transformed.drop(*drop_cols)"
      ],
      "metadata": {
        "id": "PkOeCv64L13U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyjpnwBSIVWF"
      },
      "outputs": [],
      "source": [
        "#Se eliminan registros con valores nulos\n",
        "df_clean = df_transformed_2.dropna()\n",
        "\n",
        "#Se eliminan columnas con valores nulos\n",
        "df_clean = df_clean.na.drop()\n",
        "\n",
        "#Se eliminan registros duplicados\n",
        "df_clean = df_clean.dropDuplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKNQyIIyIVWF"
      },
      "outputs": [],
      "source": [
        "df_clean.describe().toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balanceo del dataset"
      ],
      "metadata": {
        "id": "64t09Cz0OU5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.groupBy(\"DELAYED\").count().show()   # Cuenta cuantos vuelos retrasados hay (Presencia de dataset desbalanceado)"
      ],
      "metadata": {
        "id": "0J92YLSsN14X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto indica que estamos trabajando con un dataset desbalanceado, por lo que debemos utlizar métodos para balancear el dataset."
      ],
      "metadata": {
        "id": "IkeOyHSyOdJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objetivo de balanceo\n",
        "target_size = 1125\n",
        "\n",
        "# Separar las clases\n",
        "df_majority = df_clean.filter(col(\"DELAYED\") == 0)\n",
        "df_minority = df_clean.filter(col(\"DELAYED\") == 1)\n",
        "\n",
        "# Submuestreo de clase mayoritaria (clase 0)\n",
        "df_majority_sampled = df_majority.sample(False, fraction=target_size / df_majority.count(), seed=42)\n",
        "\n",
        "# Sobremuestreo de clase minoritaria (clase 1)\n",
        "ratio = int(target_size / df_minority.count())\n",
        "df_minority_oversampled = df_minority\n",
        "for _ in range(ratio - 1):\n",
        "    df_minority_oversampled = df_minority_oversampled.union(df_minority)\n",
        "\n",
        "# Agregar una fracción adicional si no es exacto\n",
        "remaining = target_size - df_minority_oversampled.count()\n",
        "if remaining > 0:\n",
        "    df_minority_oversampled = df_minority_oversampled.union(\n",
        "        df_minority.sample(withReplacement=True, fraction=remaining / df_minority.count(), seed=42)\n",
        "    )\n",
        "\n",
        "# Unir los datasets balanceados\n",
        "df_balanced = df_majority_sampled.union(df_minority_oversampled)\n",
        "\n",
        "# Verificación\n",
        "df_balanced.groupBy(\"DELAYED\").count().show()"
      ],
      "metadata": {
        "id": "fT_fNmefOsQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resumen final del dataset balanceado\n",
        "df_balanced.describe().toPandas()"
      ],
      "metadata": {
        "id": "q4EG25IRSFAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo No Supervisado**"
      ],
      "metadata": {
        "id": "zBNYh_mcIUB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecciona columnas relevantes para clustering\n",
        "clustering_cols = [\n",
        "    \"OP_CARRIER\", \"ORIGIN\", \"DEST\",\n",
        "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\",\n",
        "    \"DISTANCE\", \"CANCELLED\", \"DIVERTED\",\n",
        "    \"ARR_DELAY\"\n",
        "]\n",
        "\n",
        "df_clustering = muestra_final.select(clustering_cols)"
      ],
      "metadata": {
        "id": "Np7bGjwjIX0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar variables categóricas\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"OP_CARRIER_T\"),\n",
        "    StringIndexer(inputCol=\"ORIGIN\", outputCol=\"ORIGIN_T\"),\n",
        "    StringIndexer(inputCol=\"DEST\", outputCol=\"DEST_T\")\n",
        "]\n",
        "\n",
        "df_indexed = df_clustering\n",
        "for indexer in indexers:\n",
        "    df_indexed = indexer.fit(df_indexed).transform(df_indexed)"
      ],
      "metadata": {
        "id": "H1n4qguyJFdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columnas finales para el VectorAssembler (numéricas y las transformadas)\n",
        "feature_cols = [col for col in df_indexed.columns if col not in clustering_cols] + [\"CANCELLED\", \"DIVERTED\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"ARR_DELAY\"]\n",
        "\n",
        "# Eliminar filas con valores nulos en las columnas que se usarán para el VectorAssembler\n",
        "df_indexed_cleaned = df_indexed.dropna(subset=feature_cols)\n"
      ],
      "metadata": {
        "id": "SRLS3acjkx_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjKoEqiqBN1"
      },
      "source": [
        "# **4. Prepraración del conjunto de entrenamiento y prueba**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo Supervisado**"
      ],
      "metadata": {
        "id": "ySZwwqGZHxjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOYsN3nWIVWG"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\n",
        "        \"OP_CARRIER_T\",\"ORIGIN_T\",\"DEST_T\",\n",
        "        \"CRS_DEP_TIME\",\"CRS_ARR_TIME\",\"CRS_ELAPSED_TIME\",\"DISTANCE\",\n",
        "        \"FL_DAY_OF_WEEK\",\n",
        "        #\"ARR_DELAY\",\"DELAYED\"\n",
        "    ],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "df_vector = assembler.transform(df_balanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR7Q5XiCIVWG"
      },
      "outputs": [],
      "source": [
        "# Separación de datos de entrenamiento y prueba\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")       # Se define el valor por default del número de ejecutores\n",
        "df_train, df_val_test = df_vector.randomSplit([0.7, 0.3], seed=42)\n",
        "df_val, df_test = df_val_test.randomSplit([0.5, 0.5], seed=42)\n",
        "\n",
        "# Impresion del tamaño de las particiones\n",
        "print(f\"\"\"Total data: {df_vector.count()}\n",
        "Training data: {df_train.count()}\n",
        "Validation data: {df_val.count()}\n",
        "Test data: {df_test.count()}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo No Supervisado**"
      ],
      "metadata": {
        "id": "8DULKq4ZH32z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensamblar las características en un vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\" # Added to handle potential remaining nulls by skipping rows\n",
        ")\n",
        "\n",
        "df_vector_clustering = assembler.transform(df_indexed)\n",
        "\n",
        "# Mostrar el schema del dataframe con el vector de características\n",
        "df_vector_clustering.printSchema()"
      ],
      "metadata": {
        "id": "mxCv-ftxH7ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Selecciona columnas relevantes para clustering\n",
        "clustering_cols = [\n",
        "    \"OP_CARRIER\", \"ORIGIN\", \"DEST\",\n",
        "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\",\n",
        "    \"DISTANCE\", \"CANCELLED\", \"DIVERTED\",\n",
        "    \"ARR_DELAY\"\n",
        "]\n",
        "\n",
        "# Se asume que 'muestra_final' es el DataFrame resultante de tu sección de \"Selección de los datos\"\n",
        "df_clustering = muestra_final.select(clustering_cols)\n",
        "\n",
        "# Codificar variables categóricas\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"OP_CARRIER_T\", handleInvalid=\"keep\"),\n",
        "    StringIndexer(inputCol=\"ORIGIN\", outputCol=\"ORIGIN_T\", handleInvalid=\"keep\"),\n",
        "    StringIndexer(inputCol=\"DEST\", outputCol=\"DEST_T\", handleInvalid=\"keep\")\n",
        "]\n",
        "\n",
        "df_indexed = df_clustering\n",
        "for indexer in indexers:\n",
        "    df_indexed = indexer.fit(df_indexed).transform(df_indexed)\n"
      ],
      "metadata": {
        "id": "zn3R_xiTp6Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columnas finales para el VectorAssembler (numéricas y las transformadas)\n",
        "# Se incluyen las columnas numéricas originales y las columnas categóricas transformadas\n",
        "feature_cols_for_assembler = [\n",
        "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"ARR_DELAY\",\n",
        "    \"OP_CARRIER_T\", \"ORIGIN_T\", \"DEST_T\", \"CANCELLED\", \"DIVERTED\"\n",
        "]\n",
        "\n",
        "# Eliminar filas con valores nulos en las columnas que se usarán para el VectorAssembler\n",
        "df_indexed_cleaned = df_indexed.dropna(subset=feature_cols_for_assembler)\n",
        "\n",
        "# Ensamblar las características en un vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols_for_assembler,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\" # Ignorar filas con valores inválidos si los hay después de dropna\n",
        ")\n",
        "\n",
        "df_vector_clustering = assembler.transform(df_indexed_cleaned)\n",
        "\n",
        "# Mostrar el schema del dataframe con el vector de características\n",
        "df_vector_clustering.printSchema()\n",
        "\n",
        "# Mostrar las primeras filas del dataframe con el vector de características\n",
        "df_vector_clustering.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "mXmZ98bpqG6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS0Hxj25vTWh"
      },
      "source": [
        "# **5. Construcción de modelos de aprendizaje supervisado y no supervisado**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX60ksA0IVWG"
      },
      "source": [
        "### Modelo Supervisado - Regresión Logística"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenando el modelo"
      ],
      "metadata": {
        "id": "yId8-rsMGwBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhNFpf3EIVWG"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "# Inicializar el modelo GBTClassifier\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"DELAYED\", maxIter=10, maxBins=400) # Ajusta maxBins a un valor mayor o igual al máximo de categorías\n",
        "\n",
        "# Entrenar el modelo\n",
        "gbt_model = gbt.fit(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluando el modelo"
      ],
      "metadata": {
        "id": "GEI1NMm_G2Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = gbt_model.transform(df_test)\n",
        "\n",
        "# Inicializar el evaluador\n",
        "evaluator_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"DELAYED\")\n",
        "\n",
        "\n",
        "# Calcular el área bajo la curva ROC (AUC)\n",
        "auc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"Área bajo la curva ROC (AUC): {auc}\")\n",
        "\n",
        "\n",
        "# Inicializar el evaluador para otras métricas (MulticlassClassificationEvaluator)\n",
        "evaluator_multi = MulticlassClassificationEvaluator(labelCol=\"DELAYED\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Calcular Accuracy\n",
        "accuracy = evaluator_multi.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Calcular F1-Score\n",
        "evaluator_multi.setMetricName(\"f1\")\n",
        "f1_score = evaluator_multi.evaluate(predictions)\n",
        "print(f\"F1-Score: {f1_score}\")\n",
        "\n",
        "# Calcular Precision (para la clase positiva, asumiendo 1 es la clase positiva)\n",
        "evaluator_multi.setMetricName(\"weightedPrecision\") # Weighted average precision\n",
        "precision = evaluator_multi.evaluate(predictions)\n",
        "print(f\"Weighted Precision: {precision}\")\n",
        "\n",
        "# Calcular Recall (para la clase positiva)\n",
        "evaluator_multi.setMetricName(\"weightedRecall\") # Weighted average recall\n",
        "recall = evaluator_multi.evaluate(predictions)\n",
        "print(f\"Weighted Recall: {recall}\")"
      ],
      "metadata": {
        "id": "JCzIilt-GgdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conlusiones Modelo Supervisado"
      ],
      "metadata": {
        "id": "P8OLJc64iZdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "De acuerdo a las métricas obtenidas se puede observar un desempeño muy bueno en todas las métricas a excepción de AUC. Esto indica que el modelo no está diferenciando las clases de manera correcta. En este caso el modelo no está prediciendo de manera correcta si el viaje se retrasó o no.\n",
        "\n",
        "Una causa de esto puede ser que la calidad de la información proporcionada por las varibales, no sea la adecuada o suficiente. Añadir variables como el trafico aereoportuario o similares podrían dar mayor contexto al modelo y aumentar el puntaje.\n",
        "\n",
        "Otras maneras de mejorar el modelo, además de apoyarlo con nuevos datos, sería el realizar un ajuste de hiperparámetros o realizando un balanceo distinto que introdzuca menos ruido, por mencionar algunos.\n"
      ],
      "metadata": {
        "id": "3vE2W2_lic6B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6KLGGoEIVWG"
      },
      "source": [
        "### Modelo No Supervisado - K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenando el modelo"
      ],
      "metadata": {
        "id": "GY-gJF8LG7Oj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTJPbYoRIVWG"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Define el número de clústeres\n",
        "k = 5\n",
        "\n",
        "# Inicializa el modelo K-Means\n",
        "kmeans = KMeans(featuresCol=\"features\").setK(k).setSeed(1)\n",
        "\n",
        "# Entrena el modelo\n",
        "model = kmeans.fit(df_vector_clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluando el modelo"
      ],
      "metadata": {
        "id": "pfITbSIpHW_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza predicciones (asigna cada fila a un clúster)\n",
        "predictions = model.transform(df_vector_clustering)\n",
        "\n",
        "# Mostrar algunas predicciones\n",
        "predictions.select(\"features\", \"prediction\").show(10)\n",
        "\n",
        "# Evaluar el modelo usando Silhouette Score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(f\"Silhouette with squared Euclidean distance = {silhouette}\")\n",
        "\n",
        "# Muestra los centros de los clústeres\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "metadata": {
        "id": "-G0ILV_WHWtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5IHf_RZIVWG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusiones Modelo No Supervisado"
      ],
      "metadata": {
        "id": "mvYLYgAvHkFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se pudo realizar la agrupación de los datos de manera exitosa utilizando k-means, buscando coincidencias similares en el dataset de vuelos. Esta información nos podría ser de utlidad ya que al conocer vuelos con información similar, podríamos elegir alternativas para que un avión secundario utilice el lugar de un avión que ha sido cancelado."
      ],
      "metadata": {
        "id": "0IRu3mO9mYq4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTmMcl4Mm0Nl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4c34ZOnna3Gu",
        "MfZZ0stLmWJN",
        "ygchEdcKqIzU",
        "1qjKoEqiqBN1",
        "RS0Hxj25vTWh",
        "ToqRl7fT_fn2",
        "W4S7q0yR0Mpi",
        "pibp1LA91CP_",
        "WDIiSHvg0_hm",
        "NbhBUBKJp1MB",
        "YCkh2WfN1MC1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}